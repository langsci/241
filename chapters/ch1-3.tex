\chapter{\gkchapter{La modélisation}{Préciser l’objectif de notre étude}}\label{sec:1.3}

\section{Définition}\label{sec:1.3.0}

Ce chapitre tente de caractériser ce qu’est un modèle linguistique et précise le type de modélisation dans lequel s’inscrit notre présentation de la syntaxe. Il n’est pas essentiel à la lecture de la suite de l’ouvrage, mais permet de lever certains présupposés méthodologiques.

Nous appelons \textstyleTermes{modèle} un objet construit par le scientifique afin de \hi{simuler les propriétés de l’objet} \hi{d’étude}. L’objet d’étude est déterminé par le \textstyleTermes{cadre théorique}. Notre position théorique est de considérer que nos objets d’étude sont les langues vues comme des correspondances entre sens et textes. Ce que nous entendons par sens et textes est également déterminé par le cadre théorique (voir les sections \ref{sec:1.1.2} sur \textit{Sons et textes} et \ref{sec:1.1.4} sur \textit{Sens et intention communicative}). Le sens est représenté dans le modèle par un objet du modèle que nous appelons la \textstyleTermes{représentation sémantique}. Un modèle, dans ce cadre théorique, devra donc être capable de construire pour chaque représentation sémantique tous les textes correspondants et pour chaque texte toutes les représentations sémantiques correspondantes.

Un modèle permet de faire des \textstyleTermes{prédictions} : par exemple, pour un sens, ou plus exactement pour une représentation sémantique, qu’on n’a jamais envisagé avant, on doit être capable de construire les textes qui lui correspondent. Il en découle qu’un modèle est \textstyleTermes{falsifiable} : on peut évaluer si les prédictions du modèle correspondent à notre cadre théorique et dire par exemple si tel texte ne peut pas correspondre à tel sens et si un texte donné ne peut correspondre à aucun sens et n’est donc pas un texte de la langue. On peut ainsi construire un contre-exemple, c’est-à-dire une association sens-texte qui n’est pas prédite par le modèle, mais qui appartient à notre objet d’étude, ou, inversement, une association prédite par le modèle, mais qui n’appartient pas à la langue (telle que nous l’envisageons dans notre cadre théorique). L’\textstyleTermes{adéquation} du modèle aux données est sa capacité à faire de bonnes prédictions.

Il faut distinguer la \textstyleTermes{falsifiabilité} d’un \hi{modèle} et la \textstyleTermes{réfutabilité} de la \hi{théorie} dans laquelle se situe ce modèle. Il relève des choix théoriques de prendre en compte ou pas tel ou tel phénomène, comme par exemple les limitations mémorielles ou les erreurs de performance (voir la \sectref{sec:1.1.8} sur \textit{Langue et parole, compétence et performance}). On peut réfuter le choix théorique de prendre ou non en compte de tels phénomènes. Mais une fois le choix théorique fait, le modèle doit s’y conformer. Évidemment, la frontière entre théorie et modèle est mouvante et il est tentant d’adapter la théorie aux résultats du modèle. (Voir compléments dans le corrigé de l’exercice 2.)

La \textstyleTermes{portée} du modèle mesure l’ambition théorique du modèle. Par exemple, un modèle qui ne prend pas en compte la sémantique aura une portée moins grande qu’un modèle proposant une représentation du sens. La portée peut également concerner les données : un modèle qui prend en compte les erreurs de performance aura une portée plus grande qu’un modèle qui se limite aux productions idéales du locuteur.

La \textstyleTermes{couverture} du modèle est l’ensemble des données de l’objet d’étude qui ont été décrites par le modèle. Plus sa couverture est grande, meilleur est le modèle.

L’\textstyleTermes{économie} du modèle mesure la quantité de paramètres nécessaires à la modélisation d’un phénomène. Plus un modèle est économique, meilleur il est. Ceci repose sur l’idée qu’il y a une certaine économie dans les organismes naturels et que par le biais de la sélection naturelle les systèmes les moins économiques tendent à être éliminés.

La \textstyleTermes{flexibilité} du modèle est sa capacité à être adapté à un grand nombre d’objets par la modification d’un minimum de paramètres. Dans le cas des langues naturelles, il s’agit de pouvoir rendre compte des variations dialectales entre locuteurs, ainsi que des variations entre les différents états de langue d’un même locuteur au cours de l’apprentissage.


\loupe[sec:1.3.1]{Modèle d’une langue ou modèle de la langue}{%
    Chaque langue du monde est un objet d’étude et notre objectif premier est de construire des modèles pour chaque langue. Il y a néanmoins quelque chose de commun dans le fonctionnement des différentes langues et l’on peut, lorsqu’on a étudié suffisamment de langues particulières, être tenté d’extraire ce noyau commun que l’on peut appeler \textstyleTermes{la langue} ou la \textstyleTermes{faculté de langage}. Les propriétés communes à toutes langues sont dites \textstyleTermes{universelles}.

    Certains linguistes, comme Noam Chomsky, pensent qu’il existe une grammaire commune à toutes les langues qu’on appelle la \textstyleTermes{grammaire universelle} et que cette grammaire est innée. Il est en effet assez légitime de penser que, de même que nos mains et nos pieds se sont spécialisés pour réaliser des tâches particulières, une partie de notre cerveau, qui est l’organe du langage, doit s’être spécialisé pour cette tâche. On va alors chercher dans l’élaboration du modèle à bien distinguer \textstyleTermes{principes} et \textstyleTermes{paramètres~}: les principes sont les universaux innés de la langue que chaque être humain va paramétrer lors de l’acquisition de sa langue. Ces paramètres constituent les \textstyleTermes{idiosyncrasies} de chaque langue. Le terme vient du grec \textit{idios} ‘propre’ et \textit{synkrasis} ‘constitution’. En médecine ou en psychologie, le terme réfère souvent à un comportement propre à une personne, une réaction qui diffère de personne à personne. En linguistique, on s’intéresse peu aux spécificités langagières d’une personne, mais on s’intéresse aux spécificités d’une langue par rapport à la (faculté humaine de) langue. L’opposition entre principes et paramètres évoque une machine générique qui n’est pas construite différemment pour chaque utilisateur, mais qui permet à l’utilisateur quelques ajustements personnels. Comme pour une machine, ces ajustements personnels — les paramètres — ne sont pas libres et sont interdépendants : on ne peut choisir que certaines valeurs et le choix d’une valeur peut limiter les choix dont on dispose pour une autre valeur.

    Une modélisation d’une langue est meilleure quand elle permet facilement le paramétrage d’un sous-langage, d’un dialecte ou d’un registre de langue : on aspire à une analyse du français où un petit changement dans les paramètres — un « paramétrage » — nous donne les grammaires du français écrit journalistique, oral de conversation, de Marseille, de banlieue parisienne et même les différences d’acceptation individuelle. Un tel modèle est meilleur qu’un modèle où ces variantes du français nécessitent des descriptions tout à fait distinctes.
}

\globe[sec:1.3.1bis]{Propriétés universelles et spécificités du français}{%
Nous allons donner quelques propriétés universelles des langues, que nous contrasterons avec des propriétés spécifiques d'une langue, ici le français, puisque c'est la langue d'écriture de ce livre.

    \begin{itemize}
    \item Chaque langue parlée a un système phonologique avec un nombre fini de phonèmes. Mais le français a des voyelles nasales (\textit{en, on, in}) que la majorité des langues ne possèdent pas.
    \item Toutes les langues ont des syllabes, c’est-à-dire que les locuteurs coarticulent certains sons. Le français permet l’enchaînement de trois consonnes, comme à l’initiale du mot \textbf{\textit{str}}\textit{uctur}e, mais d’autres langues ne le permettent pas, comme par exemple le japonais ou le yoruba (une langue très diffusée en Afrique de l’Ouest).
    \item Toutes les langues ont une structure syntaxique hiérarchique. Le français possède des mots, les pronoms relatifs, qui peuvent à la fois marquer la subordination et jouer un rôle dans la subordonnée (par exemple dans \textit{le livre que Pierre lit}, \textsc{que} est à la fois un subordonnant et le complément d’objet direct du verbe \textsc{lire}), mais la majorité des langues n’ont aucun mot comme ça.
    \item Si une langue distingue singulier et pluriel pour les objets non animés, alors elle fait aussi faire cette distinction pour les objets animés, tandis que l’inverse n’est pas vrai. Il existe des langues qui possèdent un pluriel grammatical seulement pour les objets animés, comme le japonais.
    \item Le fait de posséder un système flexionnel, comme la conjugaison des verbes en français ou la déclinaison des noms en latin, n’est pas universel. En effet, le chinois n’a de flexion pour aucune classe de mots.
    \end{itemize}

    Quels paramétrages sont possibles ? Beaucoup de langues, mais pas toutes, obligent le locuteur à indiquer s’il parle d’un seul ou de plusieurs objets, mais aucune langue n’oblige le locuteur à indiquer la couleur de l’objet décrit. Dans aucune langue, le sens d’un mot dépend de la position de ce mot dans la phrase (début, 2\textsuperscript{e} place, 3\textsuperscript{e} place, etc.) et il est probable que les humains sont incapables d’apprendre une telle langue pourtant imaginable (en français, on a quelque chose qui s’apparente à ça avec la différence d’interprétation qu’il y a pour certains adjectifs entre la position avant et après le nom : \textit{un grand savant} vs \textit{un savant grand}, \textit{un jeune marié} vs \textit{un marié jeune}). Un enfant qui apprend à parler n’est donc pas obligé de peser toutes les possibilités théoriques qu’offre la communication dans l’absolu, mais il doit seulement «~paramétrer~» les choix qui lui sont offerts. La question de savoir quelles sont les contraintes innées qui président à ces choix reste un important sujet de recherche en linguistique.
}
\section{Modélisation et théorie}\label{sec:1.3.2}

Pour mieux comprendre ce qu’est la modélisation, nous allons faire un parallèle avec la physique.

\subsection{Un exemple de modélisation en physique}

Tout le monde peut observer les marées au bord de l’océan et les décrire. On verra ainsi que les marées respectent un cycle régulier d’un peu plus de 12 heures entre deux marées hautes, ainsi qu’un cycle d’environ 14 jours entre deux grandes marées et encore un cycle annuel pour l’amplitude des grandes marées, le sommet étant atteint pour les marées d’équinoxe deux fois par an. On pourra ainsi relever très précisément chaque jour l’heure des marées et leur hauteur (leur coefficient) et noter que ces hauteurs varient selon une courbe plus ou moins sinusoïdale. Jusque-là, on a \hi{décrit} le phénomène des marées, mais on ne l’a pas modélisé. Si la description est suffisamment poussée et qu’on dispose de beaucoup de données, on pourra, par des méthodes statistiques, \hi{prédire} le moment et la hauteur des marées suivantes.

On peut pousser la description jusqu’à noter une certaine corrélation entre les mouvements de la mer et les mouvements respectifs de la terre, de la lune et du soleil (la terre fait un tour sur elle-même en 24 heures, la lune fait le tour de la terre en 28 jours et la terre fait le tour du soleil en un an). Dès que l’on applique la théorie de la gravitation de Newton et que l’on considère que la lune comme le soleil attirent suffisamment les mers pour les faire bouger par rapport à la croûte terrestre, on obtient un nouveau modèle des marées. Ce modèle permet non seulement de prédire les dates et hauteurs des marées des années à l’avance, mais il permet aussi d’\hi{expliquer} le phénomène des marées et de comprendre la superposition des trois rythmes sinusoïdaux.

\subsection{La modélisation de la langue}

Considérons maintenant la langue. Tout le monde peut observer des productions langagières et les décrire. On peut compter les occurrences de chaque mot, regarder dans quels contextes elles apparaissent, avec quels autres mots avant et après, etc. On peut même noter des corrélations complexes comme la forme du verbe et la présence de tel ou tel pronom à tel endroit. Si on a pris soin de noter les circonstances dans lesquelles le texte a été énoncé, on peut pousser la description jusqu’à faire des corrélations entre le contexte d’énonciation et le texte produit. On obtient ainsi une \textstyleTermes{description} qui, combinée avec un modèle statistique, peut avoir une bonne \textstyleTermes{valeur prédictive}. Certains parlent de \textstyleTermes{modèles descriptifs} et de \textstyleTermes{modèles prédictifs}, mais, de notre point de vue, la modélisation commence réellement lorsqu’on se place dans un certain cadre théorique et que l’on fait des hypothèses sur le fonctionnement de la langue. On parle alors vraiment de \textstyleTermes{modèle} (\textstyleTermes{théorique}). Celui-ci sera d’autant meilleur qu’il aura une \textstyleTermes{valeur explicative}, c’est-à-dire qu’il nous permettra de comprendre non seulement comment nous construisons nos énoncés, mais aussi pourquoi nous devons respecter ce type de contraintes, comment nous apprenons à parler, pourquoi nous faisons tel lapsus, etc. On obtient ainsi un \textstyleTermes{modèle explicatif}.

\subsection{ Du modèle à la théorie}

On peut pousser encore plus loin la comparaison entre physique et linguistique. Le phénomène des marées n’est pas seulement une application de la théorie de la gravitation. L’histoire de la pomme de Newton est plaisante, mais il est clair qu’elle n’a été qu’un déclencheur. Newton ne cherchait pas à résoudre le problème de la chute des pommes, mais souhaitait surtout comprendre l’origine du mouvement des planètes et le phénomène des marées et c’est l’observation de ces phénomènes qui lui a permis d’élaborer et de valider la théorie de la gravitation. Dès qu’on fait l’hypothèse que les masses s’attirent, tous ces problèmes — la pomme, les planètes et les marées — trouvent une solution simple. C’est ce qu’on appelle un \textstyleTermes{raisonnement par abduction}, où on pose une hypothèse A, car elle est l’explication la plus simple à une observation C. C’est par l’observation des marées et de la chute des pommes qu’on peut émettre l’hypothèse que les masses s’attirent.

Il en va exactement de même pour la théorie linguistique : ce sont tous les phénomènes observés dans les langues qui nous permettent d’élaborer, par abduction, une théorie linguistique. C’est de cette façon que, dans le \chapref{sec:1.2} \textit{Produire un énoncé}, nous avons été amenés à faire l’hypothèse qu’il existe une structure syntaxique hiérarchique sous-jacente aux énoncés, sur laquelle s’appuient les contraintes linguistiques. Nous allons illustrer à nouveau ce point dans l’encadré qui suit sur les phénomènes d’accord.

\loupe[sec:1.3.3]{Un exemple — l’accord — de la description à l’explication}{% \label{sec:1.3.3}
    Nous allons illustrer notre propos précédent par un exemple, celui des règles d’accord.
    

\tcbsubtitle{Étape 1. Description de la concordance des formes du nom et du verbe.}
    On peut remarquer, en français, que la forme du verbe varie en fonction du sujet : \textit{Marie dort} vs \textit{Marie et Pierre dorment}, \textit{Marie finit de manger} vs \textit{Marie et Pierre finissent de manger}, etc. On poussera la description jusqu’à noter que les segments qui peuvent aller dans l’environnement {\longrule} \textit{dort} et \textit{{\longrule}} \textit{finit de manger} sont les mêmes~(\textit{Marie, elle, mon amie, la dame,} etc.) et donner un nom à cette classe : les groupes nominaux singuliers.

\tcbsubtitle{Étape 2. Énoncer la règle d’accord} Voici la règle : en français, il y a deux nombres — singulier et pluriel — et le verbe conjugué s’accorde en nombre avec son sujet. Cette règle paraît élémentaire (il y en a des plus complexes comme la règle d’accord du participe passé en français), mais elle ne l’est pas tant, car elle suppose que l’on sait définir le sujet d’un verbe et le reconnaître. L’histoire de la linguistique montre que ce ne fut pas chose facile et que définir correctement la notion de sujet, c’est déjà élaborer un début de théorie syntaxique. Modéliser cette règle par exemple pour implémenter un correcteur automatique capable d’assurer l’accord du verbe avec le sujet est encore une autre affaire. Aujourd’hui les correcteurs grammaticaux ne sont généralement pas capables de retrouver les sujets des deux verbes de la phrase «~\textbf{\textit{La pièce}} \textit{dans laquelle veulent jouer les enfants} \textbf{\textit{est}} \textit{trop petite}~» (et ils le seraient encore moins s’il y avait une faute d’orthographe).

\tcbsubtitle{Étape 3. Pourquoi il y a des règles d’accord dans les langues.} Pour bien modéliser la règle d’accord, il faut comprendre quel est son rôle dans le système. Les règles d’accord servent à marquer les relations entre les mots de manière à aider le destinataire à reconstruire la structure de l’énoncé et son sens. Il y a plusieurs manières de marquer ces relations :

    \begin{enumerate}
    \item    Une \hi{marque} qui dépend de la nature de la relation : c’est la \hi{rection}. Il peut s’agir d’un \textstyleTermes{cas}, c’est-à-dire d’une marque sur le dépendant, comme en latin (\textit{Petr}\textbf{\textit{us}} ‘Pierre’ \textrm{→} \textit{Petr}\textbf{\textit{i}} \textit{canis} ‘le chien de Pierre’), d’une préposition, c’est-à-dire d’un mot, comme en français (\textit{le chien} \textbf{\textit{de} }\textit{Pierre}) ou d’une marque sur le gouverneur comme en wolof (\textit{xaj bi} ‘le chien’, lit. chien le \textrm{→} \textit{xaj}\textbf{\textit{u}} \textit{Peer bi} ‘le chien de Pierre’).
    \item    Une \hi{marque d’accord} : cette marque peut se trouver sur le dépendant et reprendre une caractéristique du gouverneur (accord de l’adjectif avec le nom) ou l’inverse (accord du verbe avec son sujet).
    \item    Un \hi{ordre fixe} : la position du dépendant par rapport au gouverneur est très contrainte. Par exemple, en anglais, un adjectif précède le nom dont il dépend et l’objet direct suit le verbe dont il dépend.
    \item   La \hi{prosodie} : dans la phrase ambiguë \textit{Pierre regarde la fille avec un télescope}, le groupe prépositionnel \textit{avec un télescope} peut dépendre de \textit{regarde} ou de \textit{la fille}. Un contour prosodique regroupant \textit{la fille avec un télescope} désambiguïsera la phrase.
    \end{enumerate}
 

    Les différentes techniques peuvent se combiner, comme en français où le sujet possède une position contrainte (devant le verbe en général), déclenche un accord du verbe et est fléchi en cas lorsqu’il s’agit d’un pronom (\textbf{\textit{il}} \textit{dort, Marie} \textbf{\textit{le}} \textit{regarde, Marie} \textbf{\textit{lui}} \textit{parle}).

    Présenter l’accord comme nous venons de le faire, c’est prendre une position théorique : nous considérons que les mots se connectent entre eux et forment une structure hiérarchique et que les phénomènes d’accord en découlent normalement. De la même façon, la théorie de la gravitation fait l’hypothèse que les masses s’attirent et montre que les marées découlent naturellement de cette hypothèse.
}
\section{Modèle déclaratif}\label{sec:1.3.4}

Comme nous l’avons dit, un modèle linguistique doit être capable d’associer une représentation sémantique à des textes et vice versa. On distingue dans le modèle l’ensemble des \hi{connaissances} nécessaires pour effectuer cette association de la \textstyleTermes{procédure} qui permet d’activer ces connaissances et de réaliser l’association. Un modèle qui sépare connaissances et procédure est dit \textstyleTermes{déclaratif}. Sinon le modèle est dit \textstyleTermes{procédural}. On appelle généralement \textstyleTermes{grammaire formelle} un modèle déclaratif d’une langue. Le terme \textit{grammaire} employé comme ceci inclut la description du lexique de la langue (et ne correspond donc pas à l'emploi que nous faisons de ce terme dans cet ouvrage).

Supposer que les langues possèdent des modèles déclaratifs est une hypothèse forte et difficile à vérifier. Elle repose sur l’idée que les connaissances qui permettent de parler une langue et de la comprendre sont peu ou prou les mêmes. Une grammaire commune à la production et à l’analyse est dite \textstyleTermes{réversible}. On aura ensuite des procédures distinctes pour la production et l’analyse. La procédure d’analyse est plus tolérante, puisque les locuteurs comprennent des énoncés qu’ils ne seraient pas capables de produire ; les contraintes de la grammaire devront donc être relâchées en analyse.

Un modèle déclaratif est normalement un \textstyleTermes{modèle de la compétence}. En effet, les erreurs de performance doivent être imputées à des problèmes rencontrés lors de l’activation des connaissances. Néanmoins, opter pour un modèle déclaratif et une séparation entre connaissances et procédures ne signifie pas que nous rejetons la procédure hors du modèle. Notre modèle de la compétence doit être inclus dans un modèle complet de la langue prenant en compte la mise en œuvre du modèle déclaratif. Ce modèle complet est un \textstyleTermes{modèle de la performance}.

\maths[sec:1.3.5]{Modèles génératif, équatif et transductif}{%
    Noam Chomsky a révolutionné la linguistique en \citeyear{chomsky1957syntactic} dans son ouvrage \textit{Syntactic structures} en définissant son objet d'étude comme un problème mathématique. Il pose qu’une langue est l’ensemble potentiellement infini des phrases grammaticales de cette langue et qu’une grammaire est un système mathématique comportant un nombre fini de règles capable de générer l’ensemble des phrases grammaticales d’une langue. Ce courant sera appelé la \textstyleTermes{grammaire générative}. Plus tard, Chomsky renoncera à la présentation générative de la langue, mais on continuera d’appeler son école de pensée la grammaire générative, ce qui prête souvent à confusion. La naissance de la grammaire générative a coïncidé avec la naissance de la cybernétique et de l’informatique théorique et les deux mouvements se sont fortement influencés, langues naturelles et langages de programmation ayant été vus comme des objets de natures similaires.

    Dans sa version initiale, la grammaire générative ne traite pas le sens, considéré comme difficilement accessible. Les phrases, notamment dans les versions formalisées du modèle, sont traitées comme de simples suites de caractères, c’est-à-dire, pour reprendre notre terminologie, uniquement des textes. La grammaire ne se fixe alors comme principal objectif que de générer les textes d’une langue. En fait les grammaires proposées par Chomsky offrent une analyse syntaxique des phrases et définissent donc, indirectement, une correspondance entre textes et représentations syntaxiques (voir l'\encadref{sec:3.5.29} sur les \textit{Grammaires de réécriture}). D’autres chercheurs ont proposé, comme Aravind Joshi en \citeyear{joshi1975tree}, des \textstyleTermes{grammaires d’arbre} permettant de générer simultanément une phrase et sa représentation syntaxique (voir l'\encadref{sec:13-derivation} sur \textit{Lexique syntaxique et interface sémantique-syntaxe}), construisant ainsi les premières grammaires de correspondance génératives.

    Une \textstyleTermes{grammaire de correspondance} est une grammaire définissant la \hi{correspondance entre deux ensembles de structures}, par exemple des représentations sémantiques et des textes (voir l’\encadref{sec:1.3.9} sur \textit{La Théorie Sens-Texte}). On peut utiliser trois types de procédures pour définir une correspondance avec une grammaire de correspondance : procédure générative, équative ou transductive. Une \textstyleTermes{procédure générative} est une procédure qui va générer la correspondance, c’est-à-dire l’ensemble des couples en correspondance ; au lieu de générer uniquement un texte, la grammaire génère simultanément le texte et son sens. Une \textstyleTermes{procédure équative} est une procédure qui va vérifier pour chaque couple de structures qu’on lui proposera si ces structures se correspondent ; cela suppose qu’on fournisse un texte et un sens et la procédure permettra de vérifier que ce texte et ce sens peuvent être associés par la grammaire. Une \textstyleTermes{procédure transductive} est une procédure qui à chaque fois qu’on lui propose une structure est capable de construire toutes les structures qui lui correspondent ; dans ce cas, on fournit soit un texte, soit un sens et la procédure construit les sens correspondant au texte ou les textes correspondant au sens. Dans le \chapref{sec:1.2}, nous avons adopté une procédure transductive pour associer un graphe sémantique à des arbres syntaxiques.

    Ces trois types de procédure peuvent être utilisés pour présenter une même grammaire. Ce qui distingue ces trois procédures est le nombre de structures au départ : 0, 1 ou 2. Dans tous les cas, on a un couple de structures à l’arrivée. Dans la procédure générative, on part de rien et on génère simultanément les deux structures en correspondance. Dans les procédures transductives, on a une structure au départ et on produit l’autre. Dans la procédure équative, on a les deux structures dès le départ~ et on vérifie qu’elles se correspondent. Nous schématisons ci-dessous les trois procédures. Supposons qu’on veuille associer des graphes sémantiques, représentés par des {\xitsfont ☆}, à des arbres syntaxiques, représentés par des {\xitsfont\scriptsize △}. Ce qui distingue les trois procédures, ce sont les structures données au départ : nous les schématisons en noir, tandis que les structures à construire par la procédure sont en blanc.
    
    \ea procédure générative :        {\xitsfont ☆} ${\Leftrightarrow}$ {\xitsfont\scriptsize △}
    \ex procédures transductives :    {\xitsfont ★} ${\Rightarrow}$ {\xitsfont\scriptsize △}
                                       ou {\xitsfont ☆} ${\Leftarrow}$ {\xitsfont\scriptsize ▲}
    \ex procédure équative :          {\xitsfont ★} ${\Leftrightarrow}$ {\xitsfont\scriptsize ▲}
    \z
    
    Pour des raisons historiques que nous venons de rappeler, la procédure générative est souvent privilégiée. La procédure équative est généralement la procédure la plus élégante pour présenter un modèle déclaratif et elle tend à se généraliser sous le nom de \textstyleTermes{grammaires de contraintes}. Mais des trois procédures, c’est la procédure transductive qui est descriptivement la plus pertinente, car c’est ce type de procédure que les locuteurs utilisent quand ils parlent : ils doivent à partir d’un sens lui faire correspondre un texte et, à l’inverse, quand ils écoutent quelqu’un qui parle, ils partent d’un texte et doivent lui donner un sens.
}
\section{Modèle symbolique}\label{sec:1.3.6}

Une des propriétés remarquables des langues est l’utilisation d’un petit nombre de sons — les phonèmes — pour construire les signifiants de tous les éléments lexicaux de la langue. Cela met en évidence notre capacité à catégoriser, c’est-à-dire à identifier, dans la multitude de signaux de parole auxquels nous sommes confrontés lors de l’apprentissage de notre langue, un nombre fini de \textstyleTermes{symboles}, c’est-à-dire d’éléments qui ont une portée symbolique et qui possèdent une valeur d’interprétation. Plus généralement, on peut penser que les fonctions supérieures du cerveau, celles qui sont liées à la cognition et à la pensée consciente, se caractérisent par la capacité à catégoriser et à manipuler des symboles.

On appelle \textstyleTermes{modèle symbolique} ou \textstyleTermes{modèle discret} ou encore \textstyleTermes{modèle algébrique} un système basé uniquement sur la \hi{manipulation algébrique} d’un \hi{nombre fini} de \hi{symboles}. Par manipulation algébrique, on entend des opérations mathématiques permettant de combiner des configurations de symboles pour créer de nouvelles configurations (voir encadré ci-dessous sur le calcul symbolique). Un modèle qui ne «~discrétise~» pas est dit \textstyleTermes{continu}. La question se pose de savoir si un modèle linguistique doit ou non être symbolique. Les arguments contre les modèles symboliques sont assez nombreux :

\begin{itemize}
\item l’acceptabilité des énoncés n’est pas binaire : il semble y avoir un continuum entre les énoncés acceptables et les énoncés inacceptables ;
\item les catégories syntaxiques sont assez floues ; on trouve de nombreux éléments à la frontière de plusieurs catégories ;
\item les unités lexicales sont généralement polysémiques et il est difficile de déterminer combien de sens peut avoir exactement une unité lexicale ;
\item la prosodie, qui joue un rôle non négligeable dans l’expression du langage, semble difficilement catégorisable (même si la ponctuation est une forme de catégorisation de certains contours prosodiques).
\end{itemize}

Malgré cela, on arrive à fournir des modèles symboliques des langues assez satisfaisants et la plupart des modèles théoriques sont basés sur des règles manipulant des symboles.

On peut rendre compte des différents niveaux d’acceptabilité en modifiant un peu un modèle symbolique. Deux directions au moins ont été envisagées. La première, exploitée par la Théorie de l’Optimalité de Alan Prince et Paul Smolensky (\citeyear{PrinceSmolensky1993}), consiste à traiter les règles comme des contraintes éventuellement violables. De plus, les contraintes peuvent être rangées par ordre d’importance. Plus un énoncé viole de contraintes et plus ces contraintes sont importantes, moins il est acceptable.

L’autre direction consiste à pondérer les règles. On peut alors associer un score à chaque énoncé en fonction des règles qui ont permis de le produire. Cette technique est surtout utilisée en analyse pour désambiguïser : lorsque plusieurs analyses sont possibles pour un énoncé, on privilégie celle qui a le meilleur score. Des grammaires de ce type sont généralement construites en pondérant les règles selon leur probabilité d’apparition dans un corpus syntaxiquement annoté calculée à partir d’une analyse statistique de leurs occurrences. On obtient ainsi un \textstyleTermes{modèle stochastique}, dont la base reste un modèle symbolique.

Nous montrerons dans cet ouvrage les différents problèmes que pose l’identification des unités d’une langue et leur catégorisation.

\maths[sec:1.3.7]{Calcul symbolique et grammaires catégorielles}{%
    Le premier linguiste mathématicien montrant qu’on pouvait vérifier la bonne formation d’une phrase par un \textstyleTermes{calcul symbolique} est probablement le polonais Kazimierz Ajduckiewicz en \citeyear{ajduckiewicz1935syntaktische}. Voici son idée. Pour montrer que «~\textit{Pierre dort}~» est une phrase nous allons associer à chaque mot une catégorie complexe :

    \begin{itemize}
    \item \textit{Pierre} forme à lui seul un groupe nominal, nous lui associons la valeur GN ;
    \item \textit{dort} peut former une phrase P à condition qu’on le combine avec un GN : nous lui associons la valeur $\frac{\text{P}}{\text{GN}}$.
    \end{itemize}

    Nous pouvons maintenant calculer la valeur associée à \textit{Pierre dort~}:
    
    \ea
    \gll {Pierre}  {\hspace{1em}} {dort}\\
         GN   ·   $\frac{\text{P}}{\text{GN}}$\ \  =\ \    P\\
    \z
    Cette valeur est calculée en combinant les catégories associées à chaque mot et en simplifiant comme on le fait avec des fractions ordinaires. L’analyse d’une phrase devient un \textstyleTermes{calcul algébrique} similaire au calcul numérique ($a\times \nicefrac{b}{a} =b$).

    Nous avons construit un début de grammaire capable de vérifier que chaque verbe conjugué a un sujet. Une grammaire associant ainsi des catégories complexes à chaque mot est appelée une \textstyleTermes{grammaire catégorielle}. Il s’agit du premier exemple de \textstyleTermes{grammaire formelle} connu.

    Ce calcul a été ensuite repris par Yehoshua Bar-Hillel en \citeyear{barhillel1953quasi}, qui a montré que si l’on veut modéliser les contraintes d’ordre, il fallait distinguer ce qu’on combine à droite de ce qu’on combine à gauche. On va donc associer à \textit{dort} la catégorie GN{\textbackslash}P (qui se lit «~GN sous P~») indiquant que le GN avec lequel le verbe doit se combiner pour former une P doit se trouver à gauche. Analysons \textit{Pierre mange une banane} avec cette grammaire. Les mots de cette phrase ont les catégories~\textit{Pierre} := GN, \textit{banane} := D{\textbackslash}GN, \textit{une~}:= D et \textit{mange~}:= (GN{\textbackslash}P)/GN. Le calcul est le suivant :

\ea\label{ex:Bar-Hillel}
   \settowidth{\tabcolsep}{~}
   \begin{tabular}[t]{@{}c c c c c c c@{}}
    {\textit{Pierre}} &   &  {\textit{mange}} & &    {\textit{une}} & &  {\textit{banane}}\\
    GN       & · & (GN{\textbackslash}P)/GN & · & D & · & D{\textbackslash}GN\\\cmidrule{5-7}
    GN       & · & (GN{\textbackslash}P)/GN & · & \multicolumn{3}{c}{GN}\\\cmidrule{3-7}
    GN       & · & \multicolumn{5}{c}{GN{\textbackslash}P}\\\midrule
    \multicolumn{7}{c}{P}
   \end{tabular}
\z

Le diagramme est à lire en partant du haut. On associe une catégorie à chaque mot de la phrase. Les catégories D et D{\textbackslash}GN associées à \textit{une} et \textit{banane} peuvent se combiner pour donner GN, qui peut ensuite se combiner avec (GN{\textbackslash}P)/GN pour donner GN{\textbackslash}P, puis avec le GN associé à \textit{Pierre} pour donner P.
   Cette séquence de mots est bien reconnue comme une phrase par notre grammaire. De plus, la structure du calcul est un arbre (figure \ref{fig:Bar-Hillel}) que l’on peut interpréter comme la structure syntaxique de la phrase.
   
   \begin{figure}[H]
    \centering
     \caption{Arbre de constituants correspondant au calcul \REF{ex:Bar-Hillel}}
    \label{fig:Bar-Hillel}
    \begin{forest} for tree={font=\normalfont}
    [P
        [GN[\textit{Pierre}]
        ]
        [GN{\textbackslash}P
            [(GN{\textbackslash}P)/GN[\textit{mange}]
            ]
            [GN
                [D[\textit{une}]
                ]
                [D{\textbackslash}GN[\textit{banane}]
                ]
            ]
        ]
    ]
    \end{forest}
 \end{figure}

    Joachim Lambek a montré en 1958 que la règle de combinaison des catégories pouvait être interprétée comme une inférence logique. On peut en effet voir GN{\textbackslash}P comme une implication GN → P à interpréter comme «~si on me donne un GN à gauche, je formerai un P~». La règle de combinaison devient alors : «~de GN et de GN → P, je déduis P~», ce qui n’est autre que le \textit{modus ponens}, la règle de déduction de base de la logique («~de \textit{p} et de \textit{p}~→~\textit{q}, je déduis \textit{q}~»). L’analyse d’une phrase devient maintenant un \textstyleTermes{calcul logique}. Le calcul devient une \textit{preuve} que la suite de mots considérée au départ est bien une phrase. Il est intéressant de remarquer que la logique ainsi construite diffère de la logique classique, puisqu’elle est \hi{sensible aux ressources} (donner deux GN n’est pas équivalent à en donner un seul) \hi{et à l’ordre} (GN → P et P ← GN ne se comportent pas pareil, l’un attend un GN avant et l’autre après). Une telle logique, appelée \textstyleTermes{logique linéaire}, a des applications dans des domaines très éloignés de la linguistique et notamment en robotique où les actions doivent être effectuées dans un ordre bien précis.

    Dans l’\encadref{sec:3.5.30} sur les \textit{Grammaires de réécriture}, nous présenterons un autre exemple de grammaire formelle qui a marqué la deuxième moitié du 20\textsuperscript{e} siècle : les grammaires de réécriture hors-contexte introduites par Noam Chomsky en \citeyear{chomsky1957syntactic}.
}
\section{Modularité et stratification}\label{sec:1.3.8}

Un \textstyleTermes{module} est un sous-système du modèle suffisamment autonome pour effectuer seul une partie des calculs nécessaires à la production d’un énoncé. Les travaux en neurologie semblent accréditer l’idée que le cerveau possède un fonctionnement modulaire et que, lors de la production d’un énoncé, différentes aires cérébrales sont sollicitées avec des tâches différentes. Néanmoins les connaissances sur l’architecture du cerveau sont encore insuffisantes pour que se dégage une vision claire des différents modules que devrait avoir un modèle linguistique et sur la façon dont ces modules coopèrent.

Dans la suite de cet ouvrage, nous montrerons qu’il existe plusieurs types d’unités linguistiques et plusieurs modes d’organisation de ces unités. Une architecture possible est alors de considérer que chaque niveau d’organisation fournit une \textstyleTermes{strate} et que le passage du sens au texte se fait en plusieurs modules qui permettent de passer d’une strate à l’autre, l’un à la suite de l’autre. Un modèle de ce type est dit \textstyleTermes{stratificationnel}. La façon la plus simple d’utiliser un modèle stratificationnel est d’avoir une séquence de modules qui fonctionnent \hi{à la suite l’un} \hi{de l’autre} : un premier module prend en entrée le sens et fournit au deuxième module une structure complète de la strate suivante et ainsi de suite. Une telle architecture est dite \textstyleTermes{linéaire} ou en \textstyleTermes{pipeline}. Cette architecture s’oppose à une \textstyleTermes{architecture distribuée} où toutes les strates peuvent communiquer. Même dans une architecture linéaire, on peut faire que tous les modules fonctionnent simultanément et que chaque module traite les données que lui fournissent les autres modules \hi{au fur et à mesure} qu’elles arrivent. Une telle procédure est dite \textstyleTermes{incrémentale}. Le modèle que nous défendons est stratifié, modulaire, incrémental et en grande partie linéaire. Nous n’avons aujourd’hui aucun moyen de valider ou d’invalider une telle architecture.

\loupe[sec:1.3.9]{La Théorie Sens-Texte}{%
    La Théorie Sens-Texte est la théorie qui a le plus fortement inspiré les auteurs de cet ouvrage. Il s’agit d’un modèle développé autour d’Igor Mel’čuk à partir de 1965, d’abord en Union Soviétique, puis au Canada et en Europe. Ce modèle est fortement stratifié : il suppose l’existence de 5 niveaux de représentation intermédiaires entre le sens et le texte, soit 7 niveaux en tout. Le système est divisé en 6 modules permettant de passer d’un niveau à l’autre. Il s’agit d’une architecture linéaire que nous présentons ci-dessous :

    \begin{figure}[H]
    \caption{Architecture stratifiée d'un modèle Ssns-Texte}
    \begin{forest}for tree={font=\normalfont,l=2\baselineskip,edge={double,thick,{Implies[]}-{Implies[]}}}
    [\textit{sens} {=} représentation sémantique
        [représentation syntaxique profonde
            [représentation syntaxique de surface
                [représentation morphologique profonde
                    [représentation morphologique de surface
                        [représentation phonologique profonde
                            [\textit{texte} {=} représentation phonologique de surface]]]]]]]
    \end{forest}
    \end{figure}
    
    Chaque $\Leftrightarrow$ représente un module effectuant la correspondance entre deux niveaux de représentation adjacents. On trouvera une présentation et une justification de cette architecture dans le cours donné par Igor Mel’čuk en \citeyear{melcuk1997vers} au Collège de France, \textit{Vers une linguistique Sens-Texte}, disponible en ligne.

    Dans cet ouvrage, nous présentons les quatre niveaux supérieurs de représentation. Notre présentation peut être assez différente de celles que l’on trouve dans les travaux d’Igor Mel’čuk. Notre objectif est de justifier toutes les structures que nous introduirons, quitte parfois à remettre en question le statut ou la nature des représentations utilisées en Théorie Sens-Texte ou dans d’autres théories comparables, notamment la \textit{Lexical Functional Grammar} (\textit{LFG}) de Joan Bresnan et Ronald Kaplan, qui possède également une architecture stratificationnelle (voir \cite{bresnan2001lexical}).
}
\section{Modélisation des langues et ordinateur}\label{sec:1.3.10}

Le développement « à la main » d’un modèle linguistique est un travail considérable. Un dictionnaire de français courant possède 60 000 mots et l’on évalue à près d’un million le nombre d’unités lexicales si l’on y inclut les expressions figées et que l’on compte les différentes acceptions de chaque lexème. Le lexique des constructions grammaticales est encore mal connu et certainement sous-évalué́. La combinatoire de n’importe quelle description sérieuse d’un phénomène grammatical est tellement importante qu’il est difficile de voir la description dans son ensemble. L’ordinateur est alors le seul moyen pour combler cette difficulté́. Les ordinateurs possèdent aujourd’hui des capacités de calcul suffisantes pour tester la plupart des modèles imaginables (et théoriquement raisonnables). 

La validation des théories linguistiques n’est pas le seul intérêt de l’implémentation des modèles : le développement de modèles informatiques possède un réel intérêt économique, puisque la langue est au centre de toutes les activités sociales humaines. Le développement de modèles informatiques et de ressources formelles telles que lexiques, grammaires ou corpus annotés s’appelle la \terme{linguistique computationnelle}. La linguistique computationnelle est incluse dans le domaine plus vaste du \terme{traitement automatique des langues} ou \terme{TAL}, qui s’intéresse également à toutes les applications que l’on peut développer à partir de tels modèles. Les applications industrielles du TAL constituent l’\terme{ingénierie linguistique}. 
Le TAL ne nécessite pas toujours de grandes connaissances en linguistique et certaines méthodes de traitement du langage n’ont pas grand-chose à voir avec la modélisation des langues. Par exemple, pour décider en quelle langue est une page sur la Toile, le moyen le plus simple et le plus performant est de regarder la fréquence des séquences de trois lettres et de comparer avec les statistiques pour les langues que l’on souhaite reconnaître. Nul besoin de lexique et encore moins de connaissances en grammaire. 

Pour la traduction automatique, le domaine phare du TAL, on a d’abord essayé de procéder comme le font les humains : analyser le texte source pour en dégager le sens, puis reformuler ce sens dans la langue cible. La théorie Sens-Texte, présentée dans l’\encadref{sec:1.3.9} qui précède, a été initialement développée pour faire de la traduction automatique. Les traducteurs automatiques sont devenus très performants ces dernières années et pourtant ils ne cherchent plus à analyser et comprendre le texte qu’ils traduisent. La traduction repose sur de très grands corpus de textes déjà traduits. On dispose donc de corpus bilingues, avec des textes de la langue source alignés avec des textes de la langue cible. Pour traduire une phrase, on va s’appuyer sur les fragments de la phrase qui se trouvent dans le corpus bilingue pour les remplacer par un fragment correspondant dans la langue cible. Plus on a de textes déjà traduits, meilleur sera le traducteur automatique. Les traducteurs se sont améliorés dernièrement en utilisant des vecteurs de mots et des techniques d’apprentissage profond basées sur les réseaux de neurones (voir l’\encadref{sec:1.3.11} qui suit sur les \textit{Vecteurs de mots}), techniques qui sont à l'origine du bond fantastique fait par l'\terme{Intelligence Artificielle} (ou \terme{IA}) ces dernières années.

De la même manière qu'on entraîne un modèle de langue sur un grand corpus, il est imaginable de faire une étude statistique sur les marées des cent dernières années et d'en déduire de bonnes prédictions sur les marées à venir. Une telle «~modélisation~» ne nous avance pas dans la compréhension du phénomène des marées et du rôle de la gravitation (voir la \sectref{sec:1.3.2} sur \textit{Modélisation et théorie}). Les progrès considérables en TAL dûs aux architectures neuronales suggèrent que cette analogie avec la physique pourrait être trompeuse : on ne peut peut-être pas s’attendre à une modélisation en linguistique qui ressemble à la simplicité de la physique newtonienne. Les langues sont des systèmes complexes avec d’innombrables exceptions (voir l’\encadref{sec:0.0.7} sur \textit{Le lexique : un cabinet de curiosités}) et les modèles informatiques peinent à en dégager les grands principes, comme nous le faisons dans cet ouvrage. Les modèles les plus efficaces pour les applications informatiques ne sont généralement pas les modèles les plus satisfaisants du point de vue de leur explicativité. De plus ces modèles contiennent des biais qu’il est parfois difficile de contrôler en l’absence d’une lisibilité des propriétés du modèle. On peut facilement mettre en évidence ce genre de biais par l’expérience suivante : on traduit automatiquement à partir d’une langue qui n’a pas de genre dans les pronoms  (ici le turc) vers une langue qui en a (ici l’anglais) :

\ea\label{ex:1.3-traduction}
\ea \textit{O bir hemşire.}\ \ $\Longrightarrow$\ \ \textit{She is a nurse.}
\ex \textit{O bir doctor.}\ \ $\Longrightarrow$\ \ \textit{He is a doctor.}
\z
\z

Sans surprise, parce que le modèle est statistique, il va reproduire les biais de genre existant dans les textes sur lesquels il s’est entraîné : s'il y est davantage question d’infirmières que d’infirmiers et de docteurs hommes que de docteures femmes, on obtient le résultat en \REF{ex:1.3-traduction}. Ce biais qui a fait l’objet de nombreuses discussions sur les réseaux sociaux a été maintenant pris en compte, mais il y en a évidemment d’autres bien plus pernicieux qui restent cachés. Tout un champ de recherche sur les préjugés et l'équité dans l'Intelligence Artificielle s'est développé ces dernières années. Il est clair que les applications de l'IA au langage naturel ont besoin, comme pour les autres domaines d'application (on peut penser aux voitures à conduite autonome), de systèmes explicables et interprétables avant d'être lancés dans la nature. 

Disons, en conclusion, que la meilleure compréhension du fonctionnement du langage devrait permettre de faire encore progresser le TAL et que, à l’inverse, l’étude des modèles informatiques les plus performants peut donner des intuitions sur le fonctionnement du langage. L’idée générale qui se dégage des modèles informatiques actuels est que les modèles symboliques reposant sur des catégories bien délimitées atteignent rapidement leur limite et que la langue possède de nombreux phénomènes graduels, qui ne peuvent être saisis que par des modèles quantitatifs manipulant un très grand nombre de paramètres.


\loupe[sec:1.3.11]{Les vecteurs de mots}{%
L’utilisation de vecteurs de mots a permis de faire progresser de manière très significative les modèles et outils en traitement automatique des langues (TAL). L’idée est d’associer à chaque mot un point dans un espace qui rende compte du fonctionnement de ce mot et de telle façon que la distance entre les points rende compte de leur similarité de fonctionnement : plus les points sont proches, plus on suppose que les mots sont similaires.
Reste à savoir comment modéliser le fonctionnement d’un mot. La modélisation repose sur une hypothèse distributionnelle : le fonctionnement d’un mot se déduit de sa distribution, c’est-à-dire des mots qui se trouve en sa compagnie, ce que John R. \cite[11]{firth1957synopsis} a résumé en une phrase devenue célèbre : « You shall know a word by the company it keeps! » (On reconnaît un mot à ses fréquentations !). Firth fait lui-même référence aux travaux de Ludwig \cite[80,109]{wittgenstein1953philosophical} et aux citations suivantes : « Le sens d'un mot réside dans son utilisation dans la langue. […] On ne peut pas deviner comment fonctionne un mot. Il faut regarder son utilisation et en tirer des leçons. ».
  
Nous allons esquisser la mise en pratique de cette idée. On démarre avec un très grand corpus, généralement de plusieurs milliards de mots. Les vecteurs que nous calculons sont représentatifs de la distribution des mots dans ce corpus. Plus le corpus est grand, plus on aura d’informations sur la distribution de chaque mot, sur les contextes dans lesquels il peut apparaître. Pour commencer, on se fixe une taille de contexte, par exemple deux mots avant deux mots après, et de lister tous les contextes possibles. Dans la phrase «~\textit{J’ai compris que notre vie ne ressemblait à aucune autre vie sur terre.}~», le mot \textit{vie} apparaît dans les contextes (\textit{que}, \textit{notre}, \textit{ne}, \textit{ressemblait}) et (\textit{aucune}, \textit{autre}, \textit{sur}, \textit{terre}). On liste alors tous les contextes possibles dans le corpus et on associe ensuite à chaque mot une liste de nombres. Chaque nombre représente l’affinité du mot avec un des contextes du corpus. Il y a différentes façons de calculer cette affinité : cela peut être le nombre de fois où le mot apparaît dans le contexte divisé par le nombre de fois où le contexte apparaît dans le corpus et le nombre de fois où le mot apparaît dans le corpus (c’est ce qu’on appelle l’\terme{information mutuelle}, qui tient compte du fait que les choses plus fréquentes ont forcément plus de chances d’apparaître ensemble). Cette grande liste de nombre peut être interprétée comme un vecteur ou un point dans un espace multi-dimensionnel. Deux mots qui ont tendance à apparaître dans les mêmes contextes devraient donc être plus proches que des mots qui ont des distributions antinomiques. L’idée est intéressante, mais ne fonctionne pas telle quelle : les vecteurs obtenus sont bien trop grands
(l’espace a trop de dimensions) et ils sont trop disparates : des mots au fonctionnement similaire peuvent très bien ne pas être apparus dans les mêmes contextes. Il va donc falloir réduire les dimensions de l’espace et prendre en compte une autre propriété : deux mots ne sont pas seulement similaires s’ils sont proches des mêmes mots, ils sont également similaires s’ils sont proches de mots qui sont eux-mêmes similaires, lesquels sont similaires s'ils sont proches de mots similaires, et ainsi de suite. Il va donc falloir itérer le processus que nous venons de décrire, pour prendre en compte la similarité des contextes. Il existe différentes méthodes pour contrôler le nombre de dimensions et assurer la convergence vers une solution satisfaisante. Le résultat est appelé
un \hi{plongement lexical} (angl. \textit{word embedding}), puisque on « plonge » l’ensemble des mots d’un corpus dans un espace vectoriel. Le domaine a explosé en 2013 quand une équipe de Google dirigée par Tomáš Mikolov, un jeune informaticien tchèque, a proposé le plongement lexical word2vec, en utilisant des méthodes basées sur des réseaux de neurones et l’apprentissage profond (angl. \textit{deep learning}) et que les possibilités offertes par ces vecteurs ont été mises en évidence.

Une fois qu’on a plongement lexical, on peut tout simplement remplacer les mots par les vecteurs qui leur correspondent. Les systèmes de traitement du texte vont donc travailler avec des vecteurs, c’est-à-dire des tableaux de nombres, au lieu de travailler avec les mots eux-mêmes, c’est-à-dire avec des symboles, les lettres de l’alphabet. Avant l’introduction des plongements de mots, on enrichissait les systèmes de TAL en injectant des connaissances sur les mots, comme la catégorie syntaxique du mot (nom, verbe, etc.). Ceci devient superflu, car le vecteur de mot contient cette information distributionnelle : si deux mots ont la même catégorie, c’est qu’il partage des propriétés distributionnelles et leurs vecteurs devraient donc avoir certaines similarités.

Dans cet ouvrage, nous aurons constamment recours à l’analyse distributionnelle : nous considérons que le fonctionnement d’une unité du langage est fonction des unités avec lesquelles elle peut se combiner. L’objectif principal de cet ouvrage est d’étudier les combinaisons entre les unités du langage, les mots, mais aussi des unités plus petites ou plus grandes que les mots, et de montrer que ces combinaisons font partie d’un système structuré dont nous souhaitons dégager les propriétés.}


\exercices{
    \exercice{1} Notre approche de la modélisation de la langue est volontairement réductrice. En envisageant notre objet d’étude, la langue, comme une correspondance entre sens et textes, qu’est-ce que nous négligeons et ne modéliserons pas ?
    
	\exercice{2} Quelle différence de statut faisons-nous entre le sens et la représentation sémantique ?

    \exercice{3} Quelle est la différence entre falsifiabilité et réfutabilité ?

    \exercice{4} Quels seraient des modèles respectivement descriptif, prédictif et explicatif du réchauffement climatique ?

    \exercice{5} Nous avons discuté de la règle d’accord du verbe avec son sujet et du fait que cette règle reposait sur une définition préalable du sujet. Comment définiriez-vous le sujet syntaxique pour le français ?

    \exercice{6} L’énoncé \textit{La plupart sont verts} remet-il en cause la règle d’accord en nombre du verbe avec son sujet ? Comment résoudre le problème ?

    \exercice{7} Le français possède deux genres (on dit \textbf{\textit{le}} \textit{soleil} et \textbf{\textit{la}} \textit{lune} ou \textbf{\textit{une}} \textit{armoire} et \textbf{\textit{un}} \textit{tabouret}), l’allemand en possède trois (féminin, masculin et neutre), l’anglais aucun (on a juste un marquage du sexe dans les pronoms), les langues bantoues peuvent avoir jusqu’à une vingtaine de classes nominales différentes. Ce phénomène est facile à décrire, mais qu’est-ce qu’un modèle explicatif pourrait en dire ? Pourquoi les langues peuvent avoir ou ne pas avoir des classes d’accord différentes pour les noms ? Alors qu'avoir des classes d'accord différentes semble complexifier le système linguistique, pourquoi tant de langues choisissent-elles de développer et de maintenir une telle propriété ?

    \exercice{8} Pourquoi le fait qu’on puisse distinguer une question d’une assertion par la seule prosodie (une intonation montante pour «~\textit{Tu viens ?~}» et descendante pour «~\textit{Tu viens.~}») met-il en défaut une architecture linéaire comme celle de la Théorie Sens-Texte ?
}
\lecturesadditionnelles{%\label{sec:1.3.12}
    Sur la distinction entre falsifiabilité et réfutabilité, on pourra lire l’article d'Imre Lakatos, de \citeyear{lakatos1968criticism}, qui fait lui-même référence aux débats entre Popper et Kuhn suite à la réfutation de la théorie de la gravitation de Newton et à sa résolution par la théorie de la relativité d’Einstein.

    La distinction entre modèle et théorie est discutée dans le premier ouvrage de Noam Chomsky, dont la lecture est incontournable si l’on veut comprendre pourquoi cette publication a marqué un basculement de la linguistique dans le domaine des sciences. Pour Chomsky, une théorie donne un formalisme grammatical et les modèles sont les grammaires particulières que l’on peut définir avec ce formalisme.

    Le raisonnement par abduction a été dégagé par le philosophe américain, Charles S. Peirce (1839--1914). En \citeyear{peirce1903harvard}, il en donne la formulation suivante : «~The surprising fact, C, is observed ; But if A were true, C would be a matter of course ; Hence, there is reason to suspect that A is true.~».

    La Théorie Sens-Texte est présentée dans la plupart des ouvrages d’Igor Mel’čuk. Voir les ouvrages dont nous avons déjà parlé dans les trois précédents chapitres.
    
    \FurtherReading{1-3}
}
\citations{%\label{sec:1.1.15}
    \noindent {\sffamily\bfseries Citations de l'\encadref{sec:1.3.11}.}

    \noindent \citet[80,109]{wittgenstein1953philosophical} :
    
    \begin{quote}
    Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache. […]
    Wie ein Wort funktioniert kann man nicht erraten. Man muss seine Anwendung ansehen und daraus lernen.
    \end{quote}
}
\corrections{%\label{sec:1.3.13}
    \corrigé{1} En modélisant la langue comme une correspondance entre sens et textes, nous laissons de côté plusieurs choses.
    
Premièrement, nous ne prenons pas en compte ce qui se passe avant la production d’un sens. Quels sont par exemple les processus mis en jeu lorsqu’on me pose une question et que je formule une réponse ? Ceci relève pour nous de l’étude du raisonnement et dépasse notre étude. Nous ne nous intéresserons pas à la façon dont les sens sont construits. Nous nous intéressons seulement aux contraintes que la langue impose à la construction du sens et aux sens qui peuvent être réalisés par des énoncés d’une langue donnée. Nous ne modélisons pas la planification ou l'interprétation des énoncés, lesquels mettent en jeu notre connaissance du monde et de la situation d'énonciation.

Deuxièmement, nous ne prenons pas en compte la situation d’énonciation : quel est le contexte dans lequel un énoncé est produit ? Nous nous intéressons aux énoncés pour eux-mêmes. Ils ont été produits certes pour des raisons particulières, mais tant que ceci n’a pas d’impact sur la nature des énoncés nous n’en tenons pas compte. Nous n'étudions pas les interactions entre les interlocuteurs et notamment le fait que ceux-ci co-construisent un univers de discours qui repose sur un \terme{savoir partagé} (angl. \textit{common ground}, lit. terrain commun).


Troisièmement, nous ne prenons pas en compte ce qui se passe après la production d’un texte. Pour nous le texte est un objet qui est avant tout dans le cerveau du locuteur. Il en produit ensuite une image sonore (ou graphique ou gestuelle). La façon dont le son est produit à partir de la représentation qu’en a le locuteur concerne la phonétique et notamment la phonétique articulatoire et fait intervenir d’autres sciences que la linguistique, comme la biologie et la physique.


Quatrièmement, nous n’étudions pas la façon dont la correspondance est réalisée, comment le locuteur passe d’un sens à un texte quand il parle et comment il construit un sens quand il décode un texte. En particulier, le fait que le sens est généralement construit en même temps qu’il est encodé sous forme de texte n’est pas pris en compte tant que cela n’a pas d’incidence sur la forme du texte lui-même. Voir néanmoins les cas évoqués à la \sectref{sec:1.1.9} sur \textit{La planification}.

Cinquièmement, nous n’étudions pas les variations du modèle, notamment au cours de l’acquisition du langage. Comment le modèle s’élabore-t-il dans notre cerveau au cours du développement du locuteur ? Nous étudions un état de langue, une photo prise à un moment donné, tout en sachant que cet état est instable, que le contact avec de nouveaux énoncés peut faire évoluer le système. Comme on le verra, notre définition de la structure syntaxique, largement basée sur une analyse distributionnelle, permet de prendre en compte les évolutions du modèle. En effet, si de nouveaux énoncés sont pris en compte, la distribution des unités évolue en conséquence et la structure peut changer. Ce point ne sera néanmoins pas réellement traité dans la suite.

Tout en restant dans le cadre d’une correspondance sens-texte, il peut être nécessaire d’enrichir le modèle pour prendre en compte certaines productions, comme les jeux de mots ou l’ironie, où un texte peut être à double sens. Voici par exemple deux textes de Raymond Devos qui nécessite une double représentation sémantique :

\begin{enumerate}[label=(\arabic*)]
    \item \textit{Un jardinier qui sabote une pelouse est un assassin en herbe.}

    \item \textit{Je commande un demi, j'en bois la moitié, il ne m'en restait plus.}
    \end{enumerate}


    
    \corrigé{2} Tout énoncé linguistique a un sens. Le sens appartient à la langue. La représentation sémantique appartient au modèle de la langue et modélise le sens.

    \corrigé{3} La falsifiabilité est une propriété des modèles. Lorsqu’un modèle est faux (c’est-à-dire qu’il fait une mauvaise prédiction), on peut essayer de le réparer en changeant des paramètres. La réfutabilité est une propriété des théories. Pour réfuter une théorie, il faut montrer que tous les modèles qu’elle propose sont faux, ce qui est très difficile, voire impossible. La réfutabilité d’une théorie se fait donc généralement en proposant une nouvelle théorie dont l’un des modèles prend mieux en compte les données qui posent problème à la théorie précédente.

    \corrigé{4} Un modèle descriptif du réchauffement climatique serait une description des relevés de températures en divers point du globe dans les années ou les siècles qui précèdent qui montrerait une augmentation globale de la température. Une analyse statistique des variations de température permettrait de faire des prédictions sur l’évolution de la température dans les années ou siècles à venir (en considérant qu'il n'y a pas de changement drastique dans le comportement des humains). Un modèle explicatif tenterait de rechercher les causes des variations de température et de corréler ces variations avec un certain nombre de paramètres, comme la consommation d’énergie fossile par les humains, le taux de CO\textsubscript{2} dans l'air, etc. Le modèle permet alors d’affiner la prédiction en fonction de l’évolution de ces paramètres et donc de prédire que la température augmentera encore plus vite si la consommation d’énergie augmente.

    \corrigé{5} La notion de sujet syntaxique sera définie dans le \chapfuturef{17}. L’accord du verbe est une des propriétés définitoires du sujet en français, avec la position privilégiée avant le verbe ou l’emploi de pronoms comme \textit{il} ou \textit{on}.

    \corrigé{6} \textit{La plupart} est utilisé en français comme pronom pluriel masculin ou féminin. Il s’agit d’une forme figée, d’un sémantème (voir le \chapref{sec:2.3}), où \textit{la} n’est plus un marqueur du singulier féminin. Il suffit donc de déclarer \textit{la plupart} comme un pronom pluriel dont le genre dépend de son antécédent pour assurer l’accord selon les règles habituelles.

    \corrigé{7} L’origine des genres en français (et dans les autres langues indo-européennes) est un marquage des sexes pour les noms d’êtres sexués qui s’est propagé à tous les noms par régularisation du système. Pour la plupart des noms, il n’a aucune signification (même si des études montrent que l’existence des genres a une influence sur la représentation mentale et que les locuteurs d’une langue où \textit{mort} est féminin comme le français personnifieront plus naturellement la mort par une femme que par un homme à l’inverse des locuteurs d’une langue comme l’allemand où \textit{Tod} est masculin). Devoir apprendre des genres pour des noms où cela n’a pas de sens a un coût cognitif. Mais les genres vont permettre de renforcer le marquage des relations syntaxiques au travers des accords en genre. Nous avons vu que l’accord est un des moyens de marquer l’existence d’une relation syntaxique, voire la nature de cette relation (l’accord du verbe avec son sujet permet de caractériser cet élément en tant que sujet).

    \corrigé{8} L’interrogation réalisée par la seule prosodie est un exemple d’un sens qui est réalisé directement au niveau phonologique sans aucune incidence sur la syntaxe. La description de cette construction peut être faite par une correspondance directe entre sémantique et phonologie, alors qu’un modèle en pipeline obligerait à passer par la syntaxe et donc à introduire un élément fictif au niveau syntaxique.
}
